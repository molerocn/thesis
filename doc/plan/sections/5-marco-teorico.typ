#import "@local/utp-doc:1.0.0": pc

#pagebreak()

= Marco teórico
== Fundamentos teóricos

En el desarrollo de un modelo para la detección de neumonía, es importante  poder comprender las teorías y conceptos que fundamentan este desarrollo. A continuación, se presenta un análisis teórico que sustenta el proyecto, abordando aspectos esenciales como deep learning, neumonía, diagnóstico clínico, precisión de diagnóstico y tiempo de diagnóstico.

// pneumonia
Un concepto importante a abordar es la enfermedad que se busca detectar, la neumonía. Según #pc[@torres2016], se reconoce a la neumonía adquirida en la comunidad como una de las infecciones respiratorias más habituales y, si bien muchos pacientes pueden tratarse de forma ambulatoria, aquellos que requieren hospitalización presentan una mortalidad que oscila entre el 5% y el 15%, y esta puede aumentar hasta el 50% en quienes necesitan cuidados intensivos. Esta información pone en evidencia La severidad que puede alcanzar esta afección y subraya la necesidad de intervenir de manera temprana y certera para reducir complicaciones.

// diagnostico de pneumonia
#pc[@gil2005] argumenta que el diagnóstico clínico de neumonía es la fase de identificación de la enfermedad, que, sin el respaldo de una radiografía torácica, resulta inexacto, dado que los síntomas y resultados físicos no permiten distinguir de manera precisa la neumonía de otras afecciones respiratorias graves. Por esta razón, el escritor resalta la importancia de los exámenes de imagen, como la radiografía, para corroborar el diagnóstico y prevenir fallos clínicos.

// eficiencia de diagnostico
Como afirma #[@agha2022], la eficiencia en el diagnóstico no se trata simplemente de reducir costos o realizar menos pruebas, sino de usar los recursos de manera óptima para promover la salud del paciente. Aun un diagnóstico muy preciso podría no ser eficiente si el costo (financiero, físico o de tiempo clínico) de alcanzar esa precisión supera el valor clínico de detectar la enfermedad. Esta eficiencia implica equilibrar la reducción de errores diagnósticos con los riesgos y costos asociados a lograr mayor exactitud, incluyendo daños físicos por pruebas, tiempo del personal de salud y recursos del sistema. Además, destacan que decisiones diagnósticas eficientes deben considerar el contexto clínico, el valor del tratamiento tras el diagnóstico y el impacto potencial sobre el bienestar del paciente.

// precision de diagnostico
En cuanto a la precisión de diagnóstico, #pc[@balogh2015] sostiene que la precisión diagnóstica se entiende como un componente esencial de la exactitud. Un diagnóstico es considerado impreciso, y por ende inexacto, cuando carece de los detalles suficientes para describir adecuadamente la condición del paciente. Los autores destacan que el proceso diagnóstico es de naturaleza iterativa, lo que significa que su propósito es ir refinando progresivamente la comprensión del problema de salud. Este refinamiento busca reducir la incertidumbre inicial para llegar a una explicación que sea, en última instancia, más precisa y completa. De este modo, la precisión no es solo una característica deseable, sino un pilar fundamental para evitar el error diagnóstico, ya que asegura que la explicación final refleje fielmente la verdadera situación del paciente.

// tiempo de diagnostico
A continuación, #pc[@launay2016] establecen el tiempo de diagnóstico como el lapso temporal que se extiende desde la aparición de los primeros síntomas de alerta en un paciente hasta la confirmación formal del diagnóstico. El significado de este concepto supera el simple registro del tiempo, pues los autores lo descomponen en intervalos específicos que permiten un análisis más detallado, como el tiempo atribuible al paciente (desde la aparición de los primeros síntomas hasta el momento de la atención médica inicial) y el tiempo atribuible al sistema sanitario (desde el primer contacto médico hasta el diagnóstico final). El estudio de este indicador es fundamental para identificar los determinantes, ya sean del paciente o del sistema de salud, que pueden alargar este periodo. De esta manera, el tiempo de diagnóstico se convierte en una herramienta analítica para evaluar su impacto en los resultados clínicos.

// priorizacion de casos graves
En cuanto a la priorización de casos graves, #pc[@déry2020] mencionan que, la priorización de pacientes se define como el proceso de clasificar derivaciones clínicas en un orden específico según criterios explícitos, con el propósito de favorecer un acceso más justo y transparente a los servicios de salud en situaciones donde las necesidades de atención superan los recursos disponibles. Además, los autores argumentan que esta acción es beneficiosa porque permite intervenir primero en quienes presentan mayores necesidades clínicas o mayor riesgo de deterioro, reduciendo así el potencial de secuelas físicas o psicológicas derivadas de una espera prolongada.

// Deep learning
Con el propósito de definir la tecnología que a utitlizar, #pc[@goodfellow2016] lo definen como un subconjunto del machine learning centrado en algoritmos inspirados en las redes neuronales artificiales, que buscan modelar representaciones jerárquicas de los datos, permitiendo que las máquinas comprendan conceptos más complejos. De manera similar, #pc[@chollet2017] explica el deep learning como un enfoque que permite a las máquinas aprender a través de la experiencia, entendiendo el mundo mediante una jerarquía de conceptos. continuando

// Federated learning
Para poder asegurar la privacidad de los datos y entender cómo lograr este objetivo en un entorno clínico real, es importante definir el paradigma de federated learning. #pc[@yurdem2024] lo definen como una técnica de aprendizaje automático distribuido que permite que varios dispositivos o nodos colaboren en el entrenamiento de un modelo común sin compartir los datos originales. En lugar de centralizar los datos, cada participante entrena localmente y solo envía actualizaciones del modelo, lo que mejora la privacidad, seguridad, eficiencia y escalabilidad del proceso. Además, #pc[@bharati2022] complementan esta idea describiéndolo como un sistema en el que un agregador central coordina los esfuerzos de múltiples clientes para resolver problemas de aprendizaje automático. En este enfoque, los datos de entrenamiento se mantienen dispersos en los dispositivos de los usuarios, lo que permite proteger la privacidad. El entrenamiento del modelo se realiza localmente en cada dispositivo y, en lugar de compartir los datos brutos, solo se intercambian los gradientes o parámetros del modelo con el servidor central, salvaguardando así la información privada de cada participante.


Para abordar la escasez de datos etiquetados en tareas clínicas específicas, lo cual es un desafío común en el sector salud, es crucial emplear estrategias que aprovechen el conocimiento preexistente de modelos ya entrenados. #pc[@ali2023] definen el transfer learning como una técnica de aprendizaje automático cuyo objetivo es mejorar el rendimiento en una tarea específica, conocida como tarea objetivo, al utilizar el conocimiento adquirido de una tarea fuente distinta pero relacionada, para la cual sí se dispone de una mayor cantidad de datos. En esencia, en lugar de entrenar un modelo desde cero, esta técnica transfiere características, parámetros o representaciones aprendidas previamente, permitiendo que el nuevo modelo generalice de manera más efectiva y evite el sobreajuste, incluso cuando se enfrenta a un conjunto de datos limitado para la nueva tarea.

Para optimizar modelos que ya poseen una base de conocimiento general, se recurre a técnicas de ajuste especializado que permiten refinar su rendimiento sin necesidad de un reentrenamiento completo. En este contexto, #pc[@lalor2017] describen el fine-tuning como un enfoque de entrenamiento innovador donde un modelo previamente entrenado se ajusta utilizando un conjunto de datos suplementario y especializado. Esta técnica aprovecha las representaciones y el conocimiento que el modelo ya ha aprendido de un gran corpus de datos y lo especializa para una tarea más concreta mediante la actualización de sus parámetros con este nuevo conjunto de datos, que es más pequeño y específico. El enfoque principal es la mejora del desempeño y la capacidad latente del modelo en su dominio original sin afectar negativamente su capacidad de generalización.
#pc[@lalor2017]
